{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162c2925",
   "metadata": {},
   "source": [
    "### Text Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4566c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173aaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"I love programming in Python\", \"tech\"),\n",
    "    (\"Python and Java are popular programming languages\", \"tech\"),\n",
    "    (\"I enjoy watching movies and series\", \"entertainment\"),\n",
    "    (\"Cinema and film industry is booming\", \"entertainment\"),\n",
    "    (\"Machine learning and AI are future tech\", \"tech\"),\n",
    "    (\"Music concerts are fun\", \"entertainment\")\n",
    "]\n",
    "\n",
    "train_docs = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc5da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe0a25",
   "metadata": {},
   "source": [
    "### Naive Bayes(Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7971b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary and count words per class\n",
    "vocab = set()\n",
    "class_word_counts = defaultdict(Counter)\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "for text, label in train_docs:\n",
    "    words = tokenize(text)\n",
    "    class_counts[label] += 1\n",
    "    for w in words:\n",
    "        class_word_counts[label][w] += 1\n",
    "        vocab.add(w)\n",
    "\n",
    "# Total number of documents\n",
    "total_docs = sum(class_counts.values())\n",
    "\n",
    "# Predict using Naive Bayes (multinomial)\n",
    "def naive_bayes_predict(doc):\n",
    "    words = tokenize(doc)\n",
    "    best_label = None\n",
    "    best_prob = -float(\"inf\")\n",
    "    \n",
    "    for label in class_counts:\n",
    "        # P(C) = count(C)/total_docs\n",
    "        log_prob = math.log(class_counts[label]/total_docs)\n",
    "        total_words_in_class = sum(class_word_counts[label].values())\n",
    "        V = len(vocab)\n",
    "        \n",
    "        # P(w|C) = (count(w in class) + 1)/(total words in class + |V|)  -> Laplace smoothing\n",
    "        for w in words:\n",
    "            count_w = class_word_counts[label][w] if w in class_word_counts[label] else 0\n",
    "            log_prob += math.log((count_w + 1) / (total_words_in_class + V))\n",
    "        if log_prob > best_prob:\n",
    "            best_prob = log_prob\n",
    "            best_label = label\n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf6361",
   "metadata": {},
   "source": [
    "### Decision Tree using ID3 as attribute selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c752c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Word (ID3 split): python\n"
     ]
    }
   ],
   "source": [
    "def entropy(class_counts_dict):\n",
    "    total = sum(class_counts_dict.values())\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    ent = 0\n",
    "    for count in class_counts_dict.values():\n",
    "        if count == 0:\n",
    "            continue\n",
    "        p = count / total\n",
    "        ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "# Compute information gain\n",
    "def info_gain(docs, word):\n",
    "    # Parent entropy\n",
    "    label_counts = Counter([label for _, label in docs])\n",
    "    parent_entropy = entropy(label_counts)\n",
    "    \n",
    "    # Split docs by word presence\n",
    "    present = [d for d in docs if word in tokenize(d[0])]\n",
    "    absent = [d for d in docs if word not in tokenize(d[0])]\n",
    "    \n",
    "    # Weighted entropy\n",
    "    total = len(docs)\n",
    "    present_entropy = entropy(Counter([label for _, label in present]))\n",
    "    absent_entropy = entropy(Counter([label for _, label in absent]))\n",
    "    \n",
    "    weighted_entropy = (len(present)/total)*present_entropy + (len(absent)/total)*absent_entropy\n",
    "    gain = parent_entropy - weighted_entropy  # IG formula\n",
    "    return gain\n",
    "\n",
    "# Select best word\n",
    "all_words = set()\n",
    "for text, _ in train_docs:\n",
    "    all_words.update(tokenize(text))\n",
    "\n",
    "best_word_id3 = max(all_words, key=lambda w: info_gain(train_docs, w))\n",
    "print(\"Selected Word (ID3 split):\", best_word_id3)\n",
    "\n",
    "def decision_tree_id3_predict(doc):\n",
    "    words = tokenize(doc)\n",
    "    return \"tech\" if best_word_id3 in words else \"entertainment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba258a",
   "metadata": {},
   "source": [
    "### KNN(Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35fe4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building term frequency vector\n",
    "def tf_vector(text):\n",
    "    words = tokenize(text)\n",
    "    return Counter(words)\n",
    "\n",
    "# Cosine similarity formula\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot = sum(vec1.get(w,0) * vec2.get(w,0) for w in set(vec1) | set(vec2))\n",
    "    mag1 = math.sqrt(sum(v**2 for v in vec1.values()))\n",
    "    mag2 = math.sqrt(sum(v**2 for v in vec2.values()))\n",
    "    if mag1 == 0 or mag2 == 0:\n",
    "        return 0\n",
    "    return dot / (mag1 * mag2)\n",
    "\n",
    "# KNN prediction using cosine sim\n",
    "def knn_cosine_predict(doc, k=3):\n",
    "    vec_doc = tf_vector(doc)\n",
    "    similarities = []\n",
    "    \n",
    "    for text, label in train_docs:\n",
    "        vec_train = tf_vector(text)\n",
    "        sim = cosine_similarity(vec_doc, vec_train)\n",
    "        similarities.append((sim, label))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_k = [label for _, label in similarities[:k]]\n",
    "    return Counter(top_k).most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ea7e9",
   "metadata": {},
   "source": [
    "### Rochhio algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a9f50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = defaultdict(lambda: defaultdict(float))\n",
    "class_counts_words = defaultdict(int)\n",
    "\n",
    "for text, label in train_docs:\n",
    "    words = tokenize(text)\n",
    "    class_counts_words[label] += 1\n",
    "    for w in words:\n",
    "        centroids[label][w] += 1\n",
    "\n",
    "# Computing centroid = sum(word vectors)/n\n",
    "for label in centroids:\n",
    "    n = class_counts_words[label]\n",
    "    for w in centroids[label]:\n",
    "        centroids[label][w] /= n\n",
    "\n",
    "def rocchio_predict(doc):\n",
    "    words = tokenize(doc)\n",
    "    best_label = None\n",
    "    best_score = -float(\"inf\")\n",
    "    for label, vec in centroids.items():\n",
    "        score = sum(vec.get(w, 0) for w in words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_label = label\n",
    "    return best_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00c87a",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c2958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Predictions:\n",
      "'I love AI and programming' -> tech\n",
      "'Watching movies and cinema is fun' -> entertainment\n",
      "'Python machine learning' -> tech\n",
      "\n",
      "Decision Tree (ID3) Predictions:\n",
      "'I love AI and programming' -> entertainment\n",
      "'Watching movies and cinema is fun' -> entertainment\n",
      "'Python machine learning' -> tech\n",
      "\n",
      "KNN Predictions:\n",
      "'I love AI and programming' -> tech\n",
      "'Watching movies and cinema is fun' -> entertainment\n",
      "'Python machine learning' -> tech\n",
      "\n",
      "Rocchio Predictions:\n",
      "'I love AI and programming' -> tech\n",
      "'Watching movies and cinema is fun' -> entertainment\n",
      "'Python machine learning' -> tech\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\n",
    "    \"I love AI and programming\",\n",
    "    \"Watching movies and cinema is fun\",\n",
    "    \"Python machine learning\"\n",
    "]\n",
    "\n",
    "print(\"Naive Bayes Predictions:\")\n",
    "for doc in test_docs:\n",
    "    print(f\"'{doc}' -> {naive_bayes_predict(doc)}\")\n",
    "\n",
    "print(\"\\nDecision Tree (ID3) Predictions:\")\n",
    "for doc in test_docs:\n",
    "    print(f\"'{doc}' -> {decision_tree_id3_predict(doc)}\")\n",
    "\n",
    "print(\"\\nKNN Predictions:\")\n",
    "for doc in test_docs:\n",
    "    print(f\"'{doc}' -> {knn_predict(doc)}\")\n",
    "\n",
    "print(\"\\nRocchio Predictions:\")\n",
    "for doc in test_docs:\n",
    "    print(f\"'{doc}' -> {rocchio_predict(doc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
